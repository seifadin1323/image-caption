{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 1111676,
          "sourceType": "datasetVersion",
          "datasetId": 623289
        }
      ],
      "dockerImageVersionId": 30192,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "Image Caption Generator using CNN and LSTM",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'flickr8k:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F623289%2F1111676%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240623%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240623T093806Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D2e0ea0ff9b9c6ee921e573b5b2104e47735f17ebba1b467d0768f76604e05c1f85ccbb269a7627e75eec7750976980a8227157b1bca236a3785c7cf3fd2de44a966977c7600e3824f9bc53f9b4c1f07b0d928b87da0306d3c8ff1114be7f1c37482d406496ca6d6fdf2d6045999b76cfe8595c86a905ec6445ceb583691ef489da2b8c27ec026161df1d3be721f6ddb882e4d53d4e468f0b0d17b4b1edcdb50441a22eb55b340cb31371b05400645df629ccac1e64df1fb6e2b86160cde77b40014e4fcd25e078d513557e2d928d038ec1c64b31a696daf012e0bace8a232c122c4e5f5c9b3ebfa858bff4e9c167b5f06aa83258408bd62ffaa9f7216086ce8d'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "JXp-uvVOekIn"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image Caption Generator with CNN & LSTM\n",
        "\n",
        "You saw an image and your brain can easily tell what the image is about, but can a computer tell what the image is representing? Computer vision researchers worked on this a lot and they considered it impossible until now! With the advancement in Deep learning techniques, availability of huge datasets and computer power, we can build models that can generate captions for an image.\n",
        "\n",
        "This is what we are going to implement in this Python based project where we will use deep learning techniques of Convolutional Neural Networks and a type of Recurrent Neural Network (LSTM) together.\n",
        "\n",
        "## What is Image Caption Generator?\n",
        "\n",
        "Image caption generator is a task that involves computer vision and natural language processing concepts to recognize the context of an image and describe them in a natural language like English."
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "id": "K5yhaxNxekIt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Modules"
      ],
      "metadata": {
        "id": "H7nMRsRuekIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os   # handling the files\n",
        "import pickle # storing numpy features\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm # how much data is process till now\n",
        "\n",
        "from tensorflow.keras.applications.vgg16 import VGG16 , preprocess_input # extract features from image data.\n",
        "from tensorflow.keras.preprocessing.image import load_img , img_to_array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from tensorflow.keras.layers import Input , Dense , LSTM , Embedding , Dropout , add"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-22T14:20:34.968863Z",
          "iopub.execute_input": "2024-04-22T14:20:34.969518Z",
          "iopub.status.idle": "2024-04-22T14:20:34.975892Z",
          "shell.execute_reply.started": "2024-04-22T14:20:34.969484Z",
          "shell.execute_reply": "2024-04-22T14:20:34.974961Z"
        },
        "trusted": true,
        "id": "jTnLVSLKekIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Now we must set the directories to use the data"
      ],
      "metadata": {
        "id": "Zj7BNsULekIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DIR = '/kaggle/input/flickr8k'\n",
        "WORKING_DIR = '/kaggle/working'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-22T14:20:43.772151Z",
          "iopub.execute_input": "2024-04-22T14:20:43.772808Z",
          "iopub.status.idle": "2024-04-22T14:20:43.7767Z",
          "shell.execute_reply.started": "2024-04-22T14:20:43.772771Z",
          "shell.execute_reply": "2024-04-22T14:20:43.775769Z"
        },
        "trusted": true,
        "id": "NapunfsCekIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extract Image Features\n",
        "\n",
        "We have to load and restructure the model\n",
        "\n",
        "VGG-16 is a convolutional neural network that is 16 layers deep. You can load a pretrained version of the network trained on more than a million images from the ImageNet database [1]. The pretrained network can classify images into 1000 object categories, such as keyboard, mouse, pencil, and many animals."
      ],
      "metadata": {
        "id": "3aLlZ4FCekIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load vgg16 Model\n",
        "model = VGG16()\n",
        "\n",
        "# restructure model\n",
        "model = Model(inputs = model.inputs , outputs = model.layers[-2].output)\n",
        "\n",
        "# Summerize\n",
        "print(model.summary())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-22T14:20:56.100197Z",
          "iopub.execute_input": "2024-04-22T14:20:56.100907Z",
          "iopub.status.idle": "2024-04-22T14:20:58.025179Z",
          "shell.execute_reply.started": "2024-04-22T14:20:56.100872Z",
          "shell.execute_reply": "2024-04-22T14:20:58.024378Z"
        },
        "trusted": true,
        "id": "h4Aor5vfekIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Fully connected layer of the VGG16 model is not needed, just the previous layers to extract feature results.\n",
        "\n",
        "+ By preference you may include more layers, but for quicker results avoid adding the unnecessary layers."
      ],
      "metadata": {
        "id": "NFDv_BLpekIz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# extract the image features\n",
        "Now we extract the image features and load the data for preprocess"
      ],
      "metadata": {
        "id": "dIpV5WqZekI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# extract features from image\n",
        "features = {}\n",
        "directory = os.path.join(BASE_DIR, 'Images')\n",
        "\n",
        "for img_name in tqdm(os.listdir(directory)):\n",
        "    # load the image from file\n",
        "    img_path = directory + '/' + img_name\n",
        "    image = load_img(img_path, target_size=(224, 224))\n",
        "    # convert image pixels to numpy array\n",
        "    image = img_to_array(image)\n",
        "    # reshape data for model\n",
        "    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "    # preprocess image for vgg\n",
        "    image = preprocess_input(image)\n",
        "    # extract features\n",
        "    feature = model.predict(image, verbose=0)\n",
        "    # get image ID\n",
        "    image_id = img_name.split('.')[0]\n",
        "    # store feature\n",
        "    features[image_id] = feature"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-22T14:57:46.878458Z",
          "iopub.execute_input": "2024-04-22T14:57:46.87927Z",
          "iopub.status.idle": "2024-04-22T15:05:05.723825Z",
          "shell.execute_reply.started": "2024-04-22T14:57:46.879237Z",
          "shell.execute_reply": "2024-04-22T15:05:05.723071Z"
        },
        "trusted": true,
        "id": "j3g2-Vm2ekI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dictionary 'features' is created and will be loaded with the extracted features of image data\n",
        "\n",
        "**load_img(img_path, target_size=(224, 224))** - custom dimension to resize the image when loaded to the array\n",
        "\n",
        "**image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))** - reshaping the image data to preprocess in a RGB type image.\n",
        "\n",
        "**model.predict(image, verbose=0)** - extraction of features from the image\n",
        "\n",
        "**img_name.split('.')[0]** - split of the image name from the extension to load only the image name."
      ],
      "metadata": {
        "id": "ZN4wdvUCekI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# store features in pickle\n",
        "pickle.dump(features, open(os.path.join(WORKING_DIR, 'features.pkl'), 'wb'))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-22T15:05:05.725544Z",
          "iopub.execute_input": "2024-04-22T15:05:05.726081Z",
          "iopub.status.idle": "2024-04-22T15:05:06.082706Z",
          "shell.execute_reply.started": "2024-04-22T15:05:05.726039Z",
          "shell.execute_reply": "2024-04-22T15:05:06.082046Z"
        },
        "trusted": true,
        "id": "dfD10T9MekI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracted features are not stored in the disk, so re-extraction of features can extend running time\n",
        "\n",
        "Dumps and store your dictionary in a pickle for reloading it to save time"
      ],
      "metadata": {
        "id": "XdgOVdKAekI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load features from pickle\n",
        "with open(os.path.join(WORKING_DIR, 'features.pkl'), 'rb') as f:\n",
        "    features = pickle.load(f)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-22T15:05:06.083699Z",
          "iopub.execute_input": "2024-04-22T15:05:06.083908Z",
          "iopub.status.idle": "2024-04-22T15:05:06.262397Z",
          "shell.execute_reply.started": "2024-04-22T15:05:06.083882Z",
          "shell.execute_reply": "2024-04-22T15:05:06.2617Z"
        },
        "trusted": true,
        "id": "x6-zfTqXekI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load all your stored feature data to your project for quicker runtime"
      ],
      "metadata": {
        "id": "RwRCmu5lekI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Captions Data\n",
        "\n",
        "Let us store the captions data from the text file"
      ],
      "metadata": {
        "id": "w34niymPekI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(os.path.join(BASE_DIR, 'captions.txt'), 'r') as f:\n",
        "    next(f)\n",
        "    captions_doc = f.read()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-22T15:05:06.265997Z",
          "iopub.execute_input": "2024-04-22T15:05:06.266323Z",
          "iopub.status.idle": "2024-04-22T15:05:06.309165Z",
          "shell.execute_reply.started": "2024-04-22T15:05:06.266278Z",
          "shell.execute_reply": "2024-04-22T15:05:06.308477Z"
        },
        "trusted": true,
        "id": "79r0PLsHekI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now we split and append the captions data with the image"
      ],
      "metadata": {
        "id": "jk_s68JXekI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create mapping of image to captions\n",
        "mapping = {}\n",
        "# process lines\n",
        "for line in tqdm(captions_doc.split('\\n')):\n",
        "    # split the line by comma(,)\n",
        "    tokens = line.split(',')\n",
        "    if len(line) < 2:\n",
        "        continue\n",
        "    image_id, caption = tokens[0], tokens[1:]\n",
        "    # remove extension from image ID\n",
        "    image_id = image_id.split('.')[0]\n",
        "    # convert caption list to string\n",
        "    caption = \" \".join(caption)\n",
        "    # create list if needed\n",
        "    if image_id not in mapping:\n",
        "        mapping[image_id] = []\n",
        "    # store the caption\n",
        "    mapping[image_id].append(caption)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-22T15:05:06.310215Z",
          "iopub.execute_input": "2024-04-22T15:05:06.310462Z",
          "iopub.status.idle": "2024-04-22T15:05:06.722889Z",
          "shell.execute_reply.started": "2024-04-22T15:05:06.310432Z",
          "shell.execute_reply": "2024-04-22T15:05:06.722113Z"
        },
        "trusted": true,
        "id": "YuMpszBOekI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Dictionary 'mapping' is created with key as image_id and values as the corresponding caption text\n",
        "\n",
        "+ Same image may have multiple captions, **if image_id not in mapping: mapping[image_id] = []** creates a list for appending captions to the corresponding image"
      ],
      "metadata": {
        "id": "keZCFmv1ekI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Now let us see the no. of images loaded"
      ],
      "metadata": {
        "id": "4k9UadLqekI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(mapping)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-22T15:05:06.724161Z",
          "iopub.execute_input": "2024-04-22T15:05:06.724441Z",
          "iopub.status.idle": "2024-04-22T15:05:06.732727Z",
          "shell.execute_reply.started": "2024-04-22T15:05:06.724404Z",
          "shell.execute_reply": "2024-04-22T15:05:06.731935Z"
        },
        "trusted": true,
        "id": "znkAcRnDekI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess Text Data"
      ],
      "metadata": {
        "id": "VS31TJusekI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean(mapping):\n",
        "    for key, captions in mapping.items():\n",
        "        for i in range(len(captions)):\n",
        "            # take one caption at a time\n",
        "            caption = captions[i]\n",
        "            # preprocessing steps\n",
        "            # convert to lowercase\n",
        "            caption = caption.lower()\n",
        "            # delete digits, special chars, etc.,\n",
        "            caption = caption.replace('[^A-Za-z]', '')\n",
        "            # delete additional spaces\n",
        "            caption = caption.replace('\\s+', ' ')\n",
        "            # add start and end tags to the caption\n",
        "            caption = 'startseq ' + \" \".join([word for word in caption.split() if len(word)>1]) + ' endseq'\n",
        "            captions[i] = caption"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-22T15:05:06.733755Z",
          "iopub.execute_input": "2024-04-22T15:05:06.734017Z",
          "iopub.status.idle": "2024-04-22T15:05:06.742545Z",
          "shell.execute_reply.started": "2024-04-22T15:05:06.733959Z",
          "shell.execute_reply": "2024-04-22T15:05:06.741875Z"
        },
        "trusted": true,
        "id": "mKeYIl8lekI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defined to clean and convert the text for quicker process and better results"
      ],
      "metadata": {
        "id": "REotY4zuekI3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us visualize the text **before** and **after** cleaning"
      ],
      "metadata": {
        "id": "13tpj83wekI3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# before preprocess of text\n",
        "mapping['1000268201_693b08cb0e']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-22T15:05:06.743549Z",
          "iopub.execute_input": "2024-04-22T15:05:06.743869Z",
          "iopub.status.idle": "2024-04-22T15:05:06.757095Z",
          "shell.execute_reply.started": "2024-04-22T15:05:06.743823Z",
          "shell.execute_reply": "2024-04-22T15:05:06.756473Z"
        },
        "trusted": true,
        "id": "_qNLgDt9ekI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocess the text\n",
        "clean(mapping)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-22T15:05:06.758168Z",
          "iopub.execute_input": "2024-04-22T15:05:06.758464Z",
          "iopub.status.idle": "2024-04-22T15:05:06.926124Z",
          "shell.execute_reply.started": "2024-04-22T15:05:06.75843Z",
          "shell.execute_reply": "2024-04-22T15:05:06.925602Z"
        },
        "trusted": true,
        "id": "6HEuPetAekI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# after preprocess of text\n",
        "mapping['1000268201_693b08cb0e']"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-22T15:05:06.928116Z",
          "iopub.execute_input": "2024-04-22T15:05:06.928333Z",
          "iopub.status.idle": "2024-04-22T15:05:06.933313Z",
          "shell.execute_reply.started": "2024-04-22T15:05:06.928303Z",
          "shell.execute_reply": "2024-04-22T15:05:06.932601Z"
        },
        "trusted": true,
        "id": "AwByt3BZekI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Next we will store the preprocessed captions into a list"
      ],
      "metadata": {
        "id": "PQoDjzW8ekI4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_captions = []\n",
        "for key in mapping:\n",
        "    for caption in mapping[key]:\n",
        "        all_captions.append(caption)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-22T15:05:06.93439Z",
          "iopub.execute_input": "2024-04-22T15:05:06.934614Z",
          "iopub.status.idle": "2024-04-22T15:05:06.951457Z",
          "shell.execute_reply.started": "2024-04-22T15:05:06.934589Z",
          "shell.execute_reply": "2024-04-22T15:05:06.950805Z"
        },
        "trusted": true,
        "id": "sM1dVXKkekI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_captions)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-22T15:05:06.952322Z",
          "iopub.execute_input": "2024-04-22T15:05:06.9525Z",
          "iopub.status.idle": "2024-04-22T15:05:06.961446Z",
          "shell.execute_reply.started": "2024-04-22T15:05:06.952478Z",
          "shell.execute_reply": "2024-04-22T15:05:06.960718Z"
        },
        "trusted": true,
        "id": "vEpUpvQVekI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No. of unique captions stored"
      ],
      "metadata": {
        "id": "GoOv_9weekI5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10 Captions\n",
        "Let us see the first ten captions"
      ],
      "metadata": {
        "id": "BXYFi9z2ekI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_captions[:10]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-22T15:05:06.962356Z",
          "iopub.execute_input": "2024-04-22T15:05:06.962549Z",
          "iopub.status.idle": "2024-04-22T15:05:06.970506Z",
          "shell.execute_reply.started": "2024-04-22T15:05:06.962524Z",
          "shell.execute_reply": "2024-04-22T15:05:06.969824Z"
        },
        "trusted": true,
        "id": "WHoA0uFgekI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Processing of Text Data\n",
        "Now we start processing the text data"
      ],
      "metadata": {
        "id": "n2v2HS3fekI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_captions)\n",
        "vocab_size = len(tokenizer.word_index) + 1"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-22T15:05:06.971423Z",
          "iopub.execute_input": "2024-04-22T15:05:06.971627Z",
          "iopub.status.idle": "2024-04-22T15:05:07.715214Z",
          "shell.execute_reply.started": "2024-04-22T15:05:06.971593Z",
          "shell.execute_reply": "2024-04-22T15:05:07.714585Z"
        },
        "trusted": true,
        "id": "bqx8MnxoekI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-22T15:05:07.716187Z",
          "iopub.execute_input": "2024-04-22T15:05:07.716385Z",
          "iopub.status.idle": "2024-04-22T15:05:07.721629Z",
          "shell.execute_reply.started": "2024-04-22T15:05:07.716361Z",
          "shell.execute_reply": "2024-04-22T15:05:07.720794Z"
        },
        "trusted": true,
        "id": "ZXvv2VdxekI6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "No. of unique words"
      ],
      "metadata": {
        "id": "CgCW-MIwekJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get maximum length of the caption available\n",
        "max_length = max(len(caption.split()) for caption in all_captions)\n",
        "max_length"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T15:02:28.14707Z",
          "iopub.execute_input": "2024-04-16T15:02:28.147816Z",
          "iopub.status.idle": "2024-04-16T15:02:28.191978Z",
          "shell.execute_reply.started": "2024-04-16T15:02:28.147776Z",
          "shell.execute_reply": "2024-04-16T15:02:28.191272Z"
        },
        "trusted": true,
        "id": "a002r9WbekJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Finding the maximum length of the captions, used for reference for the padding sequence."
      ],
      "metadata": {
        "id": "zzzBiFFGekJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Test Split"
      ],
      "metadata": {
        "id": "1e_wdvcCekJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### After preprocessing the data now we will train, test and split"
      ],
      "metadata": {
        "id": "tfRs1YGkekJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_ids = list(mapping.keys())\n",
        "split = int(len(image_ids) * 0.90)\n",
        "train = image_ids[:split]\n",
        "test = image_ids[split:]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T15:02:30.689187Z",
          "iopub.execute_input": "2024-04-16T15:02:30.689509Z",
          "iopub.status.idle": "2024-04-16T15:02:30.694914Z",
          "shell.execute_reply.started": "2024-04-16T15:02:30.689478Z",
          "shell.execute_reply": "2024-04-16T15:02:30.693965Z"
        },
        "trusted": true,
        "id": "VHM9RoS3ekJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now we will define a batch and include the padding sequence**"
      ],
      "metadata": {
        "id": "i4TIQfzHekJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create data generator to get data in batch (avoids session crash)\n",
        "def data_generator(data_keys, mapping, features, tokenizer, max_length, vocab_size, batch_size):\n",
        "    # loop over images\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    n = 0\n",
        "    while 1:\n",
        "        for key in data_keys:\n",
        "            n += 1\n",
        "            captions = mapping[key]\n",
        "            # process each caption\n",
        "            for caption in captions:\n",
        "                # encode the sequence\n",
        "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
        "                # split the sequence into X, y pairs\n",
        "                for i in range(1, len(seq)):\n",
        "                    # split into input and output pairs\n",
        "                    in_seq, out_seq = seq[:i], seq[i]\n",
        "                    # pad input sequence\n",
        "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                    # encode output sequence\n",
        "                    out_seq = to_categorical([out_seq],num_classes=vocab_size)[0]\n",
        "                    # store the sequences\n",
        "                    X1.append(features[key][0])\n",
        "                    X2.append(in_seq)\n",
        "                    y.append(out_seq)\n",
        "            if n == batch_size:\n",
        "                X1, X2, y = np.array(X1), np.array(X2), np.array(y)\n",
        "                yield [X1, X2], y\n",
        "                X1, X2, y = list(), list(), list()\n",
        "                n = 0"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T15:02:31.647696Z",
          "iopub.execute_input": "2024-04-16T15:02:31.647988Z",
          "iopub.status.idle": "2024-04-16T15:02:31.658884Z",
          "shell.execute_reply.started": "2024-04-16T15:02:31.647954Z",
          "shell.execute_reply": "2024-04-16T15:02:31.658052Z"
        },
        "trusted": true,
        "id": "n31ise_jekJB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Padding sequence normalizes the size of all captions to the max size filling them with zeros for better results."
      ],
      "metadata": {
        "id": "ks6PGPfqekJB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Creation"
      ],
      "metadata": {
        "id": "7UJwIFB6ekJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder model\n",
        "# image feature layers\n",
        "inputs1 = Input(shape=(4096,))\n",
        "fe1 = Dropout(0.4)(inputs1)\n",
        "fe2 = Dense(256, activation='relu')(fe1)\n",
        "# sequence feature layers\n",
        "inputs2 = Input(shape=(max_length,))\n",
        "se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "se2 = Dropout(0.4)(se1)\n",
        "se3 = LSTM(256)(se2)\n",
        "\n",
        "# decoder model\n",
        "decoder1 = add([fe2, se3])\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# plot the model\n",
        "plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T15:02:33.144517Z",
          "iopub.execute_input": "2024-04-16T15:02:33.145296Z",
          "iopub.status.idle": "2024-04-16T15:02:35.332993Z",
          "shell.execute_reply.started": "2024-04-16T15:02:33.145261Z",
          "shell.execute_reply": "2024-04-16T15:02:35.332096Z"
        },
        "trusted": true,
        "id": "XCS1tnjPekJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ **shape=(4096,)** - output length of the features from the VGG model\n",
        "\n",
        "+ **Dense** - single dimension linear layer array\n",
        "\n",
        "+ **Dropout()** - used to add regularization to the data, avoiding over fitting & dropping out a fraction of the data from the layers\n",
        "\n",
        "+ **model.compile()** - compilation of the model\n",
        "\n",
        "+ **loss=’sparse_categorical_crossentropy’** - loss function for category outputs\n",
        "\n",
        "+ **optimizer=’adam’** - automatically adjust the learning rate for the model over the no. of epochs\n",
        "\n",
        "+ Model plot shows the concatenation of the inputs and outputs into a single layer\n",
        "\n",
        "+ Feature extraction of image was already done using VGG, no CNN model was needed in this step."
      ],
      "metadata": {
        "id": "42OklSF4ekJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Model\n",
        "Now let us train the model"
      ],
      "metadata": {
        "id": "4l01fkQdekJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "epochs = 1\n",
        "batch_size = 32\n",
        "steps = len(train) // batch_size\n",
        "\n",
        "for i in range(epochs):\n",
        "    # create data generator\n",
        "    generator = data_generator(train, mapping, features, tokenizer, max_length, vocab_size, batch_size)\n",
        "    # fit for one epoch\n",
        "    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T15:04:05.105817Z",
          "iopub.execute_input": "2024-04-16T15:04:05.106129Z",
          "iopub.status.idle": "2024-04-16T15:04:58.045001Z",
          "shell.execute_reply.started": "2024-04-16T15:04:05.106095Z",
          "shell.execute_reply": "2024-04-16T15:04:58.044082Z"
        },
        "trusted": true,
        "id": "zxlCjvwKekJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ **steps = len(train) // batch_size** - back propagation and fetch the next data\n",
        "\n",
        "+ Loss decreases gradually over the iterations\n",
        "\n",
        "+ Increase the no. of epochs for better results\n",
        "\n",
        "+ Assign the no. of epochs and batch size accordingly for quicker results\n",
        "\n",
        "\n",
        "### You can save the model in the working directory for reuse"
      ],
      "metadata": {
        "id": "A8LYJD_SekJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model\n",
        "model.save(WORKING_DIR+'/best_model.h5')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T15:04:58.046786Z",
          "iopub.execute_input": "2024-04-16T15:04:58.047056Z",
          "iopub.status.idle": "2024-04-16T15:04:58.173326Z",
          "shell.execute_reply.started": "2024-04-16T15:04:58.047026Z",
          "shell.execute_reply": "2024-04-16T15:04:58.17249Z"
        },
        "trusted": true,
        "id": "RgoEef94ekJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "# Path to the saved model\n",
        "model_path = WORKING_DIR + '/best_model.h5'\n",
        "\n",
        "# Load the model\n",
        "loaded_model = load_model(model_path)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T15:04:58.174576Z",
          "iopub.execute_input": "2024-04-16T15:04:58.174867Z",
          "iopub.status.idle": "2024-04-16T15:04:59.089945Z",
          "shell.execute_reply.started": "2024-04-16T15:04:58.174828Z",
          "shell.execute_reply": "2024-04-16T15:04:59.089259Z"
        },
        "trusted": true,
        "id": "26dl61SSekJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Captions for the Image"
      ],
      "metadata": {
        "id": "iF1oiFcrekJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def idx_to_word(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T15:08:12.406569Z",
          "iopub.execute_input": "2024-04-16T15:08:12.406887Z",
          "iopub.status.idle": "2024-04-16T15:08:12.412125Z",
          "shell.execute_reply.started": "2024-04-16T15:08:12.406853Z",
          "shell.execute_reply": "2024-04-16T15:08:12.411273Z"
        },
        "trusted": true,
        "id": "db0LeLTFekJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Convert the predicted index from the model into a word"
      ],
      "metadata": {
        "id": "NFcjKIt-ekJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate caption for an image\n",
        "def predict_caption(model, image, tokenizer, max_length):\n",
        "    # add start tag for generation process\n",
        "    in_text = 'startseq'\n",
        "    # iterate over the max length of sequence\n",
        "    for i in range(max_length):\n",
        "        # encode input sequence\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # pad the sequence\n",
        "        sequence = pad_sequences([sequence], max_length)\n",
        "        # predict next word\n",
        "        yhat = model.predict([image, sequence], verbose=0)\n",
        "        # get index with high probability\n",
        "        yhat = np.argmax(yhat)\n",
        "        # convert index to word\n",
        "        word = idx_to_word(yhat, tokenizer)\n",
        "        # stop if word not found\n",
        "        if word is None:\n",
        "            break\n",
        "        # append word as input for generating next word\n",
        "        in_text += \" \" + word\n",
        "        # stop if we reach end tag\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    return in_text"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T15:08:38.528696Z",
          "iopub.execute_input": "2024-04-16T15:08:38.529008Z",
          "iopub.status.idle": "2024-04-16T15:08:38.536797Z",
          "shell.execute_reply.started": "2024-04-16T15:08:38.528974Z",
          "shell.execute_reply": "2024-04-16T15:08:38.535909Z"
        },
        "trusted": true,
        "id": "DfdI1nY8ekJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Captiongenerator appending all the words for an image\n",
        "\n",
        "+ The caption starts with 'startseq' and the model continues to predict the caption until the 'endseq' appeared"
      ],
      "metadata": {
        "id": "dX3GO2eIekJE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Validation\n",
        "Now we validate the data using BLEU Score"
      ],
      "metadata": {
        "id": "1mUHNoJLekJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "# validate with test data\n",
        "actual, predicted = list(), list()\n",
        "\n",
        "for key in tqdm(test):\n",
        "    # get actual caption\n",
        "    captions = mapping[key]\n",
        "    # predict the caption for image\n",
        "    y_pred = predict_caption(model, features[key], tokenizer, max_length)\n",
        "    # split into words\n",
        "    actual_captions = [caption.split() for caption in captions]\n",
        "    y_pred = y_pred.split()\n",
        "    # append to the list\n",
        "    actual.append(actual_captions)\n",
        "    predicted.append(y_pred)\n",
        "# calcuate BLEU score\n",
        "print(\"BLEU-1: %f\" % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "print(\"BLEU-2: %f\" % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-15T17:14:54.494225Z",
          "iopub.execute_input": "2024-04-15T17:14:54.494966Z",
          "iopub.status.idle": "2024-04-15T17:20:36.067634Z",
          "shell.execute_reply.started": "2024-04-15T17:14:54.494929Z",
          "shell.execute_reply": "2024-04-15T17:20:36.066848Z"
        },
        "trusted": true,
        "id": "RX_O0wWbekJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ BLEU Score is used to evaluate the predicted text against a reference text, in a list of tokens.\n",
        "\n",
        "+ The reference text contains all the words appended from the captions data (actual_captions)\n",
        "\n",
        "+ A BLEU Score more than **0.4 is considered a good result**, for a better score increase the no. of epochs accordingly."
      ],
      "metadata": {
        "id": "VZecz7cdekJE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the Results"
      ],
      "metadata": {
        "id": "wBhoEyAHekJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "def generate_caption(image_name):\n",
        "    # load the image\n",
        "    # image_name = \"1001773457_577c3a7d70.jpg\"\n",
        "    image_id = image_name.split('.')[0]\n",
        "    img_path = os.path.join(BASE_DIR, \"Images\", image_name)\n",
        "    image = Image.open(img_path)\n",
        "    captions = mapping[image_id]\n",
        "    print('---------------------Actual---------------------')\n",
        "    for caption in captions:\n",
        "        print(caption)\n",
        "    # predict the caption\n",
        "    y_pred = predict_caption(model, features[image_id], tokenizer, max_length)\n",
        "    print('--------------------Predicted--------------------')\n",
        "    print(y_pred)\n",
        "    plt.imshow(image)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T15:08:43.877384Z",
          "iopub.execute_input": "2024-04-16T15:08:43.878027Z",
          "iopub.status.idle": "2024-04-16T15:08:43.88444Z",
          "shell.execute_reply.started": "2024-04-16T15:08:43.877991Z",
          "shell.execute_reply": "2024-04-16T15:08:43.883655Z"
        },
        "trusted": true,
        "id": "cWfCMfN4ekJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "+ Image caption generator defined\n",
        "\n",
        "+ First prints the actual captions of the image then prints a predicted caption of the image"
      ],
      "metadata": {
        "id": "gUUPcUxpekJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_caption(\"1001773457_577c3a7d70.jpg\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T15:08:44.89471Z",
          "iopub.execute_input": "2024-04-16T15:08:44.895463Z",
          "iopub.status.idle": "2024-04-16T15:08:45.564361Z",
          "shell.execute_reply.started": "2024-04-16T15:08:44.895423Z",
          "shell.execute_reply": "2024-04-16T15:08:45.563638Z"
        },
        "trusted": true,
        "id": "k80KVorbekJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_caption(\"1016887272_03199f49c4.jpg\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-16T15:09:50.265487Z",
          "iopub.execute_input": "2024-04-16T15:09:50.265809Z",
          "iopub.status.idle": "2024-04-16T15:09:50.896098Z",
          "shell.execute_reply.started": "2024-04-16T15:09:50.265776Z",
          "shell.execute_reply": "2024-04-16T15:09:50.895368Z"
        },
        "trusted": true,
        "id": "93cyGuJiekJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_caption(\"101669240_b2d3e7f17b.jpg\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-04-15T17:23:32.064768Z",
          "iopub.execute_input": "2024-04-15T17:23:32.065054Z",
          "iopub.status.idle": "2024-04-15T17:23:32.950659Z",
          "shell.execute_reply.started": "2024-04-15T17:23:32.065022Z",
          "shell.execute_reply": "2024-04-15T17:23:32.949953Z"
        },
        "trusted": true,
        "id": "eXDCjq0jekJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Thoughts\n",
        "\n",
        "+ Training the model by increasing the no. of epochs can give better and more accurate results.\n",
        "\n",
        "+ Processing large amount of data can take a lot of time and system resource.\n",
        "\n",
        "+ The no. of layers of the model can be increased if you want to process large dataset like flickr32k.\n",
        "\n",
        "\n",
        "\n",
        "**In this project , we have built an Image Caption Generator exploring the Flickr Dataset as an advanced deep learning project using different models from image extraction and text based processing.**"
      ],
      "metadata": {
        "id": "zdr5bqSHekJI"
      }
    }
  ]
}